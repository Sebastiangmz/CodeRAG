# CodeRAG Environment Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# Application Settings
# =============================================================================
DEBUG=false
APP_NAME=CodeRAG
APP_VERSION=0.1.0

# =============================================================================
# Server Settings
# =============================================================================
SERVER_HOST=0.0.0.0
SERVER_PORT=8000
SERVER_LOG_LEVEL=info
SERVER_WORKERS=1
SERVER_RELOAD=false

# =============================================================================
# Model Settings
# =============================================================================
# LLM Provider: "local", "openai", "groq", "anthropic", "openrouter", "together"
# Use "local" for on-device inference, or a cloud provider for faster responses
MODEL_LLM_PROVIDER=local

# API Configuration (required for non-local providers)
# Get your API key from:
#   - Groq (FREE, fast): https://console.groq.com/keys
#   - OpenAI: https://platform.openai.com/api-keys
#   - Anthropic: https://console.anthropic.com/settings/keys
#   - OpenRouter: https://openrouter.ai/keys
#   - Together: https://api.together.xyz/settings/api-keys
MODEL_LLM_API_KEY=
MODEL_LLM_API_BASE=

# LLM Model Configuration
# For local: Qwen/Qwen2.5-Coder-3B-Instruct (recommended for 8GB VRAM)
# For groq: llama-3.3-70b-versatile (default), mixtral-8x7b-32768
# For openai: gpt-4o-mini (default), gpt-4o, gpt-4-turbo
# For anthropic: claude-3-5-sonnet-20241022 (default)
# For openrouter: anthropic/claude-3.5-sonnet (default)
# For together: meta-llama/Llama-3.3-70B-Instruct-Turbo (default)
MODEL_LLM_NAME=Qwen/Qwen2.5-Coder-3B-Instruct
MODEL_LLM_MAX_NEW_TOKENS=1024
MODEL_LLM_TEMPERATURE=0.1
MODEL_LLM_TOP_P=0.95
MODEL_LLM_USE_4BIT=true
MODEL_LLM_DEVICE_MAP=auto

# Embedding Configuration
MODEL_EMBEDDING_NAME=nomic-ai/nomic-embed-text-v1.5
MODEL_EMBEDDING_DIMENSION=768
MODEL_EMBEDDING_BATCH_SIZE=32
# Use CPU for embeddings to leave GPU free for LLM (required for 8GB VRAM)
MODEL_EMBEDDING_DEVICE=cpu

# =============================================================================
# Vector Store Settings
# =============================================================================
VECTORSTORE_PERSIST_DIRECTORY=./data/chroma_db
VECTORSTORE_COLLECTION_NAME=coderag_chunks
VECTORSTORE_DISTANCE_METRIC=cosine
VECTORSTORE_ANONYMIZED_TELEMETRY=false

# =============================================================================
# Ingestion Settings
# =============================================================================
INGESTION_REPOS_CACHE_DIR=./data/repos
INGESTION_MAX_FILE_SIZE_KB=500
INGESTION_DEFAULT_BRANCH=main
INGESTION_CHUNK_SIZE=1500
INGESTION_CHUNK_OVERLAP=200

# =============================================================================
# Retrieval Settings
# =============================================================================
RETRIEVAL_DEFAULT_TOP_K=5
RETRIEVAL_MAX_TOP_K=20
RETRIEVAL_SIMILARITY_THRESHOLD=0.3

# =============================================================================
# HuggingFace Settings (Optional)
# =============================================================================
# HF_HOME=./data/hf_cache
# TRANSFORMERS_CACHE=./data/hf_cache
# HF_TOKEN=your_token_here  # Only needed for gated models
