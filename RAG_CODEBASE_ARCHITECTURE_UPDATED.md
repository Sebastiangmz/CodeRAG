# RAG Grounded para Repositorios de CÃ³digo - Documento de Arquitectura

## InformaciÃ³n del Proyecto

- **Nombre del Proyecto**: CodeRAG (RAG Grounded para Codebases)
- **Tipo**: Sistema de Retrieval-Augmented Generation especializado en repositorios de cÃ³digo
- **Lenguaje Principal de Soporte**: Python (con capacidad de extensiÃ³n a otros lenguajes)
- **Fase Actual**: MVP - Modo Q&A
- **EvoluciÃ³n Planificada**: Modo Patch/Diff (generaciÃ³n de cambios de cÃ³digo)
- **Interfaz**: Gradio montado en FastAPI
- **Deployment**: Docker Compose (100% reproducible local)
- **Alcance MVP**: Solo repositorios pÃºblicos de GitHub (sin autenticaciÃ³n)
- **Hardware Objetivo**: GPU NVIDIA RTX 4060 8GB VRAM

---

## 1. VisiÃ³n General del Proyecto

### 1.1 QuÃ© es RAG Grounded

RAG (Retrieval-Augmented Generation) funciona "aumentando" el prompt del usuario con pasajes recuperados desde una base de conocimiento externa. El LLM genera la respuesta usando ese contexto recuperado en lugar de inventar informaciÃ³n.

**Beneficio principal**: Reducir alucinaciones porque el modelo se apoya en informaciÃ³n recuperada (documentaciÃ³n, cÃ³digo fuente) en vez de generar respuestas sin fundamento.

### 1.2 Objetivo del Sistema

Construir un asistente de Q&A sobre repositorios de cÃ³digo que:

1. **Responda preguntas** sobre el cÃ³digo de forma precisa
2. **Cite las fuentes** (archivo + rango de lÃ­neas) para cada afirmaciÃ³n
3. **Rechace responder** cuando no hay evidencia en los chunks recuperados
4. **Se adapte** a casi cualquier repositorio, con optimizaciÃ³n especial para Python

### 1.3 Casos de Uso MVP (Modo Q&A)

El usuario puede hacer preguntas como:

- "Â¿DÃ³nde se define la clase `UserAuthentication`?"
- "Â¿QuÃ© hace la funciÃ³n `process_payment()`?"
- "Â¿CÃ³mo se configura el logger en este proyecto?"
- "Â¿QuÃ© dependencias usa este mÃ³dulo?"
- "Â¿CuÃ¡l es el flujo de datos en el endpoint `/api/users`?"

**Criterio de Ã©xito**: El sistema encuentra el lugar correcto en el repo y la respuesta estÃ¡ sustentada por el contexto recuperado con citas verificables.

### 1.4 EvoluciÃ³n Futura (Modo Patch/Diff)

En fases posteriores, el usuario podrÃ¡ pedir:

- "Agrega soporte para paginaciÃ³n en este endpoint"
- "Refactoriza esta funciÃ³n para usar async/await"
- "Arregla este bug en el manejo de errores"

El sistema responderÃ¡ con un **diff aplicable** que indica quÃ© lÃ­neas agregar/quitar en quÃ© archivo.

**Criterio de Ã©xito adicional**: El cambio debe compilar, pasar tests, y estar justificado por el contexto del repo.

---

## 2. Modelos de IA y Licencias

### 2.1 Modelos Seleccionados

Este proyecto utiliza modelos **100% locales y gratuitos** que corren en una RTX 4060 (8GB VRAM) usando cuantizaciÃ³n 4-bit.

| Rol | Modelo | Licencia | JustificaciÃ³n |
|-----|--------|----------|---------------|
| **LLM Generator** | Qwen2.5-Coder-7B-Instruct | Apache 2.0 | Especializado en cÃ³digo, buen rendimiento en Q&A sobre repos |
| **Embeddings** | nomic-embed-text v1.5 | Apache 2.0 | Embeddings de calidad, open source, bajo consumo |
| **Alternativa ligera** | Llama-3.2-3B-Instruct | Llama 3.2 Community | MÃ¡s rÃ¡pido, menos VRAM (revisar licencia) |

### 2.2 ConfiguraciÃ³n de Modelos

```yaml
# configs/models.yaml
llm:
  model_name: "Qwen/Qwen2.5-Coder-7B-Instruct"
  quantization: "4bit"  # GPTQ o bitsandbytes
  max_new_tokens: 1024
  temperature: 0.1
  device_map: "auto"

embeddings:
  model_name: "nomic-ai/nomic-embed-text-v1.5"
  device: "cuda"
  normalize_embeddings: true
  dimensions: 768
```

### 2.3 Licencias y Permisos

Ambos modelos principales estÃ¡n bajo **Apache License 2.0**, que permite:
- âœ… Uso comercial y personal
- âœ… ModificaciÃ³n y redistribuciÃ³n
- âœ… Crear obras derivadas (como adaptadores LoRA)
- âš ï¸ Requiere mantener avisos de copyright si redistribuyes

**Links oficiales de licencias (pinnear estas versiones):**
- [Qwen2.5-Coder-7B-Instruct LICENSE](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE)
- [nomic-embed-text v1.5 LICENSE](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)

### 2.4 Mejores PrÃ¡cticas para Repositorio PÃºblico

Dado que el objetivo es tener el cÃ³digo pÃºblico para portafolio, se deben seguir estas reglas:

#### âœ… SÃ publicar en el repo:
- Todo el cÃ³digo fuente (pipeline, UI, API)
- Scripts de entrenamiento (`train_qlora.py`)
- Configuraciones (YAML/JSON con hiperparÃ¡metros)
- README con mÃ©tricas y resultados
- Docker files y docker-compose
- Datasets de evaluaciÃ³n (preguntas de prueba)

#### âŒ NO publicar en el repo:
- Checkpoints de modelos (`*.safetensors`, `*.bin`)
- Adaptadores LoRA entrenados (`adapter_model.safetensors`, `adapter_config.json`)
- Ãndices vectoriales pre-construidos (ChromaDB dumps)
- Datasets procesados de repos de terceros
- Archivos `.env` con API keys

#### Estructura de .gitignore recomendada:

```gitignore
# Artefactos de modelos y fine-tuning
artifacts/
checkpoints/
adapters/
*.safetensors
*.bin
*.pt
*.pth

# Base de datos vectorial
vectorstore/
chroma_db/
data/

# Cache de repositorios clonados
repos/

# Entorno y secretos
.env
.env.local
*.env

# Python
__pycache__/
*.pyc
.pytest_cache/
.mypy_cache/
*.egg-info/
venv/
.venv/

# IDE
.vscode/
.idea/
```

### 2.5 Â¿QuÃ© son los Adaptadores LoRA?

Los **adaptadores LoRA** (Low-Rank Adaptation) son pequeÃ±os conjuntos de pesos que se entrenan para adaptar un modelo base a una tarea especÃ­fica, sin modificar los pesos originales del modelo.

**En este proyecto:**
- El modelo base (Qwen2.5-Coder) se descarga desde Hugging Face y se mantiene congelado
- El fine-tuning con QLoRA produce archivos pequeÃ±os (~100MB): `adapter_model.safetensors` + `adapter_config.json`
- En inferencia, el adaptador se "enchufa" al modelo base para producir el comportamiento entrenado

**DistribuciÃ³n de artefactos derivados:**
- Si subes los archivos `adapter_*` a GitHub, estÃ¡s redistribuyendo un "artefacto derivado" del modelo base
- Esto activa obligaciones de licencia adicionales
- **RecomendaciÃ³n**: El pipeline descarga el modelo base y genera adaptadores localmente; no se suben al repo pÃºblico

---

## 3. Arquitectura del Sistema

### 3.1 Pipeline Principal

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              PIPELINE RAG                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  CLONAR/ â”‚   â”‚ FILTRAR  â”‚   â”‚ CHUNKING â”‚   â”‚EMBEDDINGSâ”‚   â”‚  ÃNDICE  â”‚  â”‚
â”‚  â”‚  CARGAR  â”‚â”€â”€â–¶â”‚ ARCHIVOS â”‚â”€â”€â–¶â”‚          â”‚â”€â”€â–¶â”‚  nomic   â”‚â”€â”€â–¶â”‚ ChromaDB â”‚  â”‚
â”‚  â”‚   REPO   â”‚   â”‚          â”‚   â”‚          â”‚   â”‚          â”‚   â”‚          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚ CONSULTA â”‚   â”‚RECUPERAR â”‚   â”‚ GENERAR  â”‚                                 â”‚
â”‚  â”‚ USUARIO  â”‚â”€â”€â–¶â”‚  TOP-K   â”‚â”€â”€â–¶â”‚  Qwen2.5 â”‚â”€â”€â–¶ Respuesta con citas          â”‚
â”‚  â”‚          â”‚   â”‚  CHUNKS  â”‚   â”‚  Coder   â”‚                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Componentes del Sistema

#### 3.2.1 MÃ³dulo de Carga de Repositorios

**Responsabilidad**: Clonar o cargar repositorios Git y convertir archivos a documentos procesables.

**ImplementaciÃ³n sugerida**:
- Usar `GitLoader` de LangChain o implementaciÃ³n propia
- Soportar: URL de Git, path local, o archivo ZIP

**Salida**: Lista de documentos con contenido y metadatos (path, nombre, extensiÃ³n)

#### 3.2.2 MÃ³dulo de Filtrado de Archivos

**Responsabilidad**: Filtrar archivos relevantes y excluir basura (binarios, lockfiles, dependencias vendorizadas).

**Reglas por defecto**:

```yaml
include_patterns:
  - "README*"
  - "docs/**"
  - "src/**"
  - "*.py"
  - "*.md"
  - "*.rst"
  - "pyproject.toml"
  - "requirements*.txt"
  - "setup.py"
  - "setup.cfg"
  - "*.yaml"
  - "*.yml"
  - "*.json"  # Solo configs, no data dumps
  - "*.toml"

exclude_patterns:
  - ".git/**"
  - "venv/**"
  - ".venv/**"
  - "__pycache__/**"
  - "dist/**"
  - "build/**"
  - "node_modules/**"
  - "*.egg-info/**"
  - ".tox/**"
  - ".pytest_cache/**"
  - ".mypy_cache/**"
  - "*.png"
  - "*.jpg"
  - "*.jpeg"
  - "*.gif"
  - "*.ico"
  - "*.pdf"
  - "*.zip"
  - "*.tar.gz"
  - "*.whl"
  - "*.pyc"
  - "*.pyo"
  - "*.so"
  - "*.dylib"
  - "*.dll"
  - "poetry.lock"
  - "package-lock.json"
  - "yarn.lock"
  - "Pipfile.lock"
```

**ConfiguraciÃ³n**: Permitir override por proyecto vÃ­a archivo `.coderagignore` o config.

#### 3.2.3 MÃ³dulo de Chunking

**Responsabilidad**: Dividir archivos en chunks coherentes para indexaciÃ³n.

**Estrategia dual**:

1. **Chunking SemÃ¡ntico (Python)**: Usar AST para extraer unidades coherentes
2. **Chunking por Texto (Fallback)**: Para archivos sin parser disponible

##### Chunking SemÃ¡ntico para Python

**TecnologÃ­a**: Tree-sitter (py-tree-sitter)

**Unidades a extraer**:
- `function_definition` â†’ Funciones completas
- `class_definition` â†’ Clases completas (o mÃ©todos individuales si son muy grandes)
- `decorated_definition` â†’ Funciones/clases con decoradores

**Metadatos por chunk**:
```python
{
    "file_path": "src/auth/handlers.py",
    "chunk_type": "function",
    "name": "authenticate_user",
    "start_line": 45,
    "end_line": 78,
    "parent_class": "AuthHandler",  # Si aplica
    "decorators": ["@require_auth", "@log_access"],
    "signature": "def authenticate_user(self, username: str, password: str) -> bool:",
    "docstring": "Authenticate user with credentials...",
    "imports_used": ["hashlib", "datetime"],
    "commit_hash": "abc123..."  # Opcional
}
```

##### Chunking por Texto (Fallback)

**ParÃ¡metros**:
- `chunk_size`: 1000-1500 tokens (configurable)
- `chunk_overlap`: 100-200 tokens
- Preservar lÃ­mites de lÃ­nea cuando sea posible

**Metadatos por chunk**:
```python
{
    "file_path": "docs/installation.md",
    "chunk_type": "text",
    "start_line": 1,
    "end_line": 45,
    "section_title": "Installation Guide",  # Si se puede inferir
    "commit_hash": "abc123..."
}
```

#### 3.2.4 MÃ³dulo de Embeddings

**Responsabilidad**: Generar embeddings vectoriales para cada chunk.

**Modelo seleccionado**: `nomic-embed-text v1.5`

```python
from sentence_transformers import SentenceTransformer

# InicializaciÃ³n
embedding_model = SentenceTransformer(
    "nomic-ai/nomic-embed-text-v1.5",
    trust_remote_code=True,
    device="cuda"
)

# Generar embeddings
embeddings = embedding_model.encode(
    texts,
    normalize_embeddings=True,
    show_progress_bar=True
)
```

**Consideraciones**:
- DimensiÃ³n: 768
- El modelo debe ser consistente entre indexaciÃ³n y consulta
- Normalizar embeddings para usar similitud coseno

#### 3.2.5 Base de Datos Vectorial

**Responsabilidad**: Almacenar embeddings e Ã­ndices para bÃºsqueda por similitud.

**TecnologÃ­a seleccionada**: ChromaDB

**JustificaciÃ³n**:
- Simple de configurar y usar
- Buen DX (Developer Experience)
- Ideal para desarrollo local y MVP
- Persistencia en disco fÃ¡cil de configurar
- IntegraciÃ³n nativa con LangChain

**ConfiguraciÃ³n bÃ¡sica**:

```python
import chromadb
from chromadb.config import Settings

# Cliente persistente
client = chromadb.PersistentClient(
    path="./data/chroma_db",
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True
    )
)

# Crear o obtener colecciÃ³n
collection = client.get_or_create_collection(
    name="coderag_chunks",
    metadata={"hnsw:space": "cosine"}  # Similitud coseno
)
```

**Ãndice debe almacenar**:
- Vector embedding
- Contenido del chunk (texto)
- Todos los metadatos del chunk
- ID Ãºnico del chunk

#### 3.2.6 MÃ³dulo de RecuperaciÃ³n (Retrieval)

**Responsabilidad**: Dado un query, recuperar los K chunks mÃ¡s relevantes.

**ParÃ¡metros**:
- `top_k`: 5-10 chunks (configurable)
- `similarity_threshold`: 0.7 mÃ­nimo (opcional, para filtrar chunks poco relevantes)

**Estrategias de mejora** (para fases posteriores):
- Hybrid search (vector + keyword BM25)
- Reranking con modelo cross-encoder
- Query expansion/reformulation

#### 3.2.7 MÃ³dulo de GeneraciÃ³n

**Responsabilidad**: Generar respuesta basada en chunks recuperados, con citas.

**Modelo**: Qwen2.5-Coder-7B-Instruct (cuantizado 4-bit)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# ConfiguraciÃ³n de cuantizaciÃ³n
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# Cargar modelo
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B-Instruct",
    trust_remote_code=True
)
```

**Formato de citas**:
```
[archivo:lÃ­nea_inicio-lÃ­nea_fin]
```

Ejemplos:
- `[src/auth/handlers.py:45-78]`
- `[README.md:12-25]`
- `[docs/api.md:100-115]`

**Prompt del sistema** (comportamiento requerido):

```
Eres un asistente de cÃ³digo que responde preguntas sobre un repositorio.

REGLAS ESTRICTAS:
1. Solo responde basÃ¡ndote en los chunks de cÃ³digo/documentaciÃ³n proporcionados
2. Cada afirmaciÃ³n importante DEBE incluir una cita en formato [archivo:lÃ­neas]
3. Si la informaciÃ³n NO estÃ¡ en los chunks, responde: "No encontrÃ© informaciÃ³n sobre esto en el repositorio indexado"
4. NO inventes cÃ³digo, funciones, o archivos que no estÃ©n en los chunks
5. Si la pregunta es ambigua, pide clarificaciÃ³n
6. Responde de forma concisa y estructurada

FORMATO DE RESPUESTA:
- Respuestas en bullets cuando sea apropiado
- Cita despuÃ©s de cada afirmaciÃ³n relevante
- Incluye snippets de cÃ³digo solo si son cortos y relevantes
```

---

## 4. Interfaz de Usuario (Gradio + FastAPI)

### 4.1 Arquitectura de la UI

La interfaz se implementa con **Gradio montado dentro de FastAPI** usando `gradio.mount_gradio_app()`. Esto permite tener UI y API en el mismo proceso, accesible en `http://localhost:8000`.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FastAPI App                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Gradio UI         â”‚    â”‚        API REST                 â”‚ â”‚
â”‚  â”‚   /gradio           â”‚    â”‚        /api/v1/*                â”‚ â”‚
â”‚  â”‚                     â”‚    â”‚                                 â”‚ â”‚
â”‚  â”‚  - Indexar repos    â”‚    â”‚  - POST /repos/index            â”‚ â”‚
â”‚  â”‚  - Chat Q&A         â”‚    â”‚  - POST /query                  â”‚ â”‚
â”‚  â”‚  - Ver progreso     â”‚    â”‚  - GET /repos                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 DiseÃ±o de la Interfaz

#### Panel de IndexaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ðŸ“¦ INDEXAR REPOSITORIO                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  URL del Repositorio (GitHub):                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ https://github.com/owner/repo                                â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Opciones Avanzadas (expandible) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                  â”‚
â”‚  Branch:              Top-K:           Filtros:                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â˜‘ Incluir tests           â”‚
â”‚  â”‚ main         â”‚    â”‚ 5        â”‚     â˜ Solo documentaciÃ³n      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚      ðŸš€ INDEXAR         â”‚                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  65% - Procesando chunks...  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Panel de Chat Q&A

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ðŸ’¬ PREGUNTAR SOBRE EL CÃ“DIGO                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Repositorio activo: langchain (342 chunks indexados)           â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Â¿DÃ³nde se define la clase BaseRetriever?                    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚      ðŸ” PREGUNTAR       â”‚                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ RESPUESTA:                                                   â”‚â”‚
â”‚  â”‚                                                              â”‚â”‚
â”‚  â”‚ La clase `BaseRetriever` se define en el mÃ³dulo de          â”‚â”‚
â”‚  â”‚ retrievers [src/retrievers/base.py:23-89]:                  â”‚â”‚
â”‚  â”‚                                                              â”‚â”‚
â”‚  â”‚ - Es una clase abstracta que define la interfaz comÃºn       â”‚â”‚
â”‚  â”‚   [src/retrievers/base.py:25-30]                            â”‚â”‚
â”‚  â”‚ - El mÃ©todo principal es `get_relevant_documents()`         â”‚â”‚
â”‚  â”‚   [src/retrievers/base.py:45-67]                            â”‚â”‚
â”‚  â”‚                                                              â”‚â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚â”‚
â”‚  â”‚ ðŸ“Ž EVIDENCIA:                                                â”‚â”‚
â”‚  â”‚ â€¢ src/retrievers/base.py (lÃ­neas 23-89)                     â”‚â”‚
â”‚  â”‚ â€¢ src/retrievers/__init__.py (lÃ­neas 5-12)                  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 Componentes Gradio

```python
import gradio as gr
from fastapi import FastAPI

app = FastAPI()

# DefiniciÃ³n de la interfaz Gradio
with gr.Blocks(title="CodeRAG - Q&A sobre Repositorios") as demo:
    gr.Markdown("# ðŸ” CodeRAG - Asistente de CÃ³digo con RAG")
    
    with gr.Tab("ðŸ“¦ Indexar"):
        repo_url = gr.Textbox(
            label="URL del Repositorio (GitHub)",
            placeholder="https://github.com/owner/repo"
        )
        
        with gr.Accordion("Opciones Avanzadas", open=False):
            branch = gr.Textbox(label="Branch", value="main")
            top_k = gr.Slider(minimum=1, maximum=20, value=5, label="Top-K chunks")
            include_tests = gr.Checkbox(label="Incluir tests", value=False)
            docs_only = gr.Checkbox(label="Solo documentaciÃ³n", value=False)
        
        index_btn = gr.Button("ðŸš€ Indexar", variant="primary")
        index_progress = gr.Progress()
        index_output = gr.Textbox(label="Estado", interactive=False)
        
        index_btn.click(
            fn=index_repository,
            inputs=[repo_url, branch, top_k, include_tests, docs_only],
            outputs=[index_output]
        )
    
    with gr.Tab("ðŸ’¬ Preguntar"):
        repo_status = gr.Markdown("*No hay repositorio indexado*")
        question = gr.Textbox(
            label="Tu pregunta",
            placeholder="Â¿DÃ³nde se define la funciÃ³n X?"
        )
        ask_btn = gr.Button("ðŸ” Preguntar", variant="primary")
        
        answer_output = gr.Markdown(label="Respuesta")
        evidence_output = gr.JSON(label="Evidencia (chunks recuperados)")
        
        ask_btn.click(
            fn=ask_question,
            inputs=[question],
            outputs=[answer_output, evidence_output]
        )

# Montar Gradio en FastAPI
app = gr.mount_gradio_app(app, demo, path="/")
```

### 4.4 Flujo de Usuario

#### Flujo 1: Indexar Repositorio

```
1. Usuario abre http://localhost:8000
2. En tab "Indexar", pega URL: https://github.com/owner/repo
3. (Opcional) Ajusta branch, top-k, filtros
4. Click en "Indexar"
5. Sistema muestra barra de progreso:
   - "Clonando repositorio..." (10%)
   - "Filtrando archivos..." (20%)
   - "Procesando chunks..." (40-80%)
   - "Generando embeddings..." (80-95%)
   - "Guardando Ã­ndice..." (95-100%)
6. Mensaje: "âœ… Repositorio indexado: 342 chunks"
7. Tab "Preguntar" se habilita
```

#### Flujo 2: Hacer Pregunta

```
1. Usuario va a tab "Preguntar"
2. Ve: "Repositorio activo: repo-name (342 chunks)"
3. Escribe pregunta: "Â¿CÃ³mo se configura el logging?"
4. Click en "Preguntar"
5. Sistema:
   a. Genera embedding de la pregunta (nomic-embed)
   b. Busca top-k chunks similares (ChromaDB)
   c. Construye prompt con contexto
   d. Qwen2.5-Coder genera respuesta con citas
6. Muestra respuesta + evidencia (archivos, lÃ­neas, snippets)
```

### 4.5 Manejo de Progreso con Gradio

Gradio soporta `gr.Progress` para mostrar avance en tareas largas:

```python
def index_repository(repo_url: str, branch: str, progress=gr.Progress()):
    """Indexa un repositorio con feedback de progreso."""
    
    progress(0, desc="Validando URL...")
    validate_github_url(repo_url)
    
    progress(0.1, desc="Clonando repositorio...")
    repo_path = clone_repository(repo_url, branch)
    
    progress(0.2, desc="Filtrando archivos...")
    files = filter_files(repo_path)
    
    progress(0.3, desc="Procesando chunks...")
    chunks = []
    for i, file in enumerate(files):
        chunks.extend(chunk_file(file))
        progress(0.3 + (0.4 * i / len(files)), desc=f"Chunking: {file.name}")
    
    progress(0.7, desc="Generando embeddings (nomic-embed)...")
    embeddings = generate_embeddings(chunks)
    
    progress(0.9, desc="Guardando Ã­ndice (ChromaDB)...")
    save_index(embeddings, chunks)
    
    progress(1.0, desc="Â¡Completado!")
    return f"âœ… Repositorio indexado: {len(chunks)} chunks"
```

---

## 5. Infraestructura Docker

### 5.1 Arquitectura de Contenedores

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Docker Compose                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    coderag-app                               â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚  â”‚  â”‚  FastAPI    â”‚  â”‚   Gradio    â”‚  â”‚   RAG Pipeline      â”‚ â”‚â”‚
â”‚  â”‚  â”‚  :8000      â”‚  â”‚   UI        â”‚  â”‚   (indexing/query)  â”‚ â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                              â”‚                                   â”‚
â”‚                              â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    VolÃºmenes (locales, no en Git)           â”‚â”‚
â”‚  â”‚  ./data:/app/data       (ChromaDB - Ã­ndices vectoriales)    â”‚â”‚
â”‚  â”‚  ./repos:/app/repos     (repos clonados - cache)            â”‚â”‚
â”‚  â”‚  ./models:/app/models   (cache modelos HuggingFace)         â”‚â”‚
â”‚  â”‚  ./adapters:/app/adapters (LoRA adapters - si se entrenan)  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Dockerfile (con soporte CUDA)

```dockerfile
# Dockerfile
FROM nvidia/cuda:12.1-runtime-ubuntu22.04

WORKDIR /app

# Instalar Python y dependencias del sistema
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    git \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Symlink python
RUN ln -s /usr/bin/python3.11 /usr/bin/python

# Copiar requirements e instalar dependencias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar cÃ³digo fuente
COPY src/ ./src/
COPY configs/ ./configs/

# Crear directorios para datos persistentes
RUN mkdir -p /app/data /app/repos /app/models /app/adapters

# Variables de entorno
ENV PYTHONPATH=/app/src
ENV DATA_DIR=/app/data
ENV REPOS_DIR=/app/repos
ENV HF_HOME=/app/models
ENV ADAPTERS_DIR=/app/adapters

# Puerto de la aplicaciÃ³n
EXPOSE 8000

# Comando de inicio
CMD ["uvicorn", "coderag.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5.3 Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  coderag:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      # Persistencia de Ã­ndices vectoriales (ChromaDB)
      - ./data:/app/data
      # Cache de repositorios clonados
      - ./repos:/app/repos
      # Cache de modelos de HuggingFace (evita re-descargar)
      - ./models:/app/models
      # Adaptadores LoRA entrenados (si aplica)
      - ./adapters:/app/adapters
    environment:
      # ConfiguraciÃ³n de modelos (locales, sin API keys)
      - LLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct
      - EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
      # ConfiguraciÃ³n
      - LOG_LEVEL=INFO
      - GRADIO_SERVER_NAME=0.0.0.0
      # CUDA
      - NVIDIA_VISIBLE_DEVICES=all
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  data:
  repos:
  models:
  adapters:
```

### 5.4 Archivo .env

```bash
# .env.example (copiar a .env)
# NO INCLUIR ESTE ARCHIVO EN GIT SI TIENE DATOS SENSIBLES

# Modelos locales (no requieren API keys)
LLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# ConfiguraciÃ³n de retrieval
TOP_K_DEFAULT=5
SIMILARITY_THRESHOLD=0.7

# ConfiguraciÃ³n de chunking
CHUNK_SIZE=1500
CHUNK_OVERLAP=150

# ConfiguraciÃ³n de generaciÃ³n
MAX_NEW_TOKENS=1024
TEMPERATURE=0.1

# HuggingFace (opcional, para descargas privadas)
# HF_TOKEN=hf_...

# Logging
LOG_LEVEL=INFO
```

### 5.5 Comandos de Uso

```bash
# Construir y levantar el servicio (primera vez descarga modelos ~15GB)
docker compose up --build

# Levantar en segundo plano
docker compose up -d

# Ver logs
docker compose logs -f coderag

# Parar el servicio
docker compose down

# Limpiar datos (reset completo - NO borra modelos descargados)
docker compose down -v
rm -rf ./data ./repos

# Limpiar TODO incluyendo modelos (re-descargarÃ¡ ~15GB)
docker compose down -v
rm -rf ./data ./repos ./models ./adapters
```

### 5.6 Persistencia y Cache

**Volumen `./data`**: Almacena los Ã­ndices vectoriales (ChromaDB). Permite que un repositorio indexado persista entre reinicios del contenedor.

**Volumen `./repos`**: Cache de repositorios clonados. Evita re-clonar el mismo repo si ya existe localmente.

**Volumen `./models`**: Cache de modelos de HuggingFace. **IMPORTANTE**: Evita re-descargar ~15GB cada vez que se reconstruye el contenedor.

**Volumen `./adapters`**: Almacena adaptadores LoRA entrenados. Se mantienen fuera de Git.

```python
# LÃ³gica de cache de repos
def get_or_clone_repo(repo_url: str, branch: str) -> Path:
    """Obtiene repo del cache o lo clona si no existe."""
    repo_id = hash_repo_url(repo_url, branch)
    cache_path = Path(os.environ["REPOS_DIR"]) / repo_id
    
    if cache_path.exists():
        # Actualizar repo existente (git pull)
        update_repo(cache_path)
        return cache_path
    else:
        # Clonar nuevo repo
        return clone_repo(repo_url, branch, cache_path)
```

---

## 6. Requisitos de Hardware

### 6.1 ConfiguraciÃ³n MÃ­nima (MVP)

| Componente | EspecificaciÃ³n | Notas |
|------------|----------------|-------|
| **GPU** | NVIDIA RTX 4060 8GB | CuantizaciÃ³n 4-bit requerida |
| **RAM** | 16GB | 32GB recomendado para repos grandes |
| **Storage** | 50GB SSD | Modelos (~15GB) + Ã­ndices + repos |
| **CUDA** | 12.1+ | Requerido para inferencia GPU |

### 6.2 Uso de VRAM Estimado

| Componente | VRAM |
|------------|------|
| Qwen2.5-Coder-7B (4-bit) | ~4.5GB |
| nomic-embed-text v1.5 | ~0.5GB |
| Overhead CUDA | ~1GB |
| **Total** | **~6GB** |

**Margen disponible**: ~2GB para batches de embeddings y contextos largos.

### 6.3 Alternativa Ligera

Si la VRAM es insuficiente, usar Llama-3.2-3B-Instruct:

| Componente | VRAM |
|------------|------|
| Llama-3.2-3B (4-bit) | ~2GB |
| nomic-embed-text v1.5 | ~0.5GB |
| Overhead | ~1GB |
| **Total** | **~3.5GB** |

---

## 7. Decisiones de Alcance MVP

### 7.1 Incluido en MVP

| Feature | DescripciÃ³n | JustificaciÃ³n |
|---------|-------------|---------------|
| Repos pÃºblicos | Solo GitHub pÃºblico | Simplifica auth, suficiente para demo |
| Interfaz Gradio | UI web con botones | UX simple, rÃ¡pido de implementar |
| Q&A con citas | Preguntas â†’ respuestas citadas | Core value proposition |
| Chunking Python | AST-aware para .py | Diferenciador tÃ©cnico |
| Docker local | 100% reproducible | Portafolio profesional |
| Persistencia | Ãndices en volumen | No re-indexar cada vez |
| Modelos locales | Qwen2.5-Coder + nomic-embed | Sin costos de API |

### 7.2 Excluido del MVP (Futuro)

| Feature | RazÃ³n de exclusiÃ³n | Fase futura |
|---------|-------------------|-------------|
| Repos privados | Requiere GITHUB_TOKEN, complejidad auth | v1.1 |
| Modo Patch/Diff | Requiere validaciÃ³n de cÃ³digo, tests | v2.0 |
| Multi-repo | Un Ã­ndice por repo es mÃ¡s simple | v1.2 |
| Fine-tuning | Primero validar RAG base funciona | v1.5 |
| Reranking | OptimizaciÃ³n, no core | v1.3 |
| Auth de usuarios | No necesario para demo local | v2.0 |

### 7.3 ValidaciÃ³n de URL GitHub

```python
import re
from urllib.parse import urlparse

def validate_github_url(url: str) -> tuple[str, str]:
    """
    Valida URL de GitHub y extrae owner/repo.
    
    Args:
        url: URL del repositorio (ej: https://github.com/owner/repo)
        
    Returns:
        Tuple de (owner, repo_name)
        
    Raises:
        ValueError: Si la URL no es vÃ¡lida o no es de GitHub
    """
    parsed = urlparse(url)
    
    # Validar dominio
    if parsed.netloc not in ("github.com", "www.github.com"):
        raise ValueError(f"Solo se soportan repos de GitHub. Dominio recibido: {parsed.netloc}")
    
    # Extraer owner/repo del path
    path_parts = parsed.path.strip("/").split("/")
    if len(path_parts) < 2:
        raise ValueError(f"URL invÃ¡lida. Formato esperado: https://github.com/owner/repo")
    
    owner, repo = path_parts[0], path_parts[1]
    
    # Limpiar .git si existe
    repo = repo.removesuffix(".git")
    
    # Validar caracteres
    if not re.match(r"^[\w\-\.]+$", owner) or not re.match(r"^[\w\-\.]+$", repo):
        raise ValueError(f"Nombre de owner o repo contiene caracteres invÃ¡lidos")
    
    return owner, repo
```

---

## 8. Esquema de Datos

### 8.1 Documento (Pre-chunking)

```python
@dataclass
class Document:
    content: str
    metadata: DocumentMetadata

@dataclass
class DocumentMetadata:
    file_path: str          # Ruta relativa al root del repo
    file_name: str          # Nombre del archivo
    extension: str          # ExtensiÃ³n (.py, .md, etc.)
    size_bytes: int         # TamaÃ±o del archivo
    last_modified: datetime # Ãšltima modificaciÃ³n
    repo_url: str           # URL del repositorio (si aplica)
    commit_hash: str        # Hash del commit (opcional)
    branch: str             # Branch (opcional)
```

### 8.2 Chunk (Post-chunking)

```python
@dataclass
class Chunk:
    id: str                 # UUID Ãºnico
    content: str            # Contenido del chunk
    embedding: List[float]  # Vector embedding (768 dims para nomic)
    metadata: ChunkMetadata

@dataclass
class ChunkMetadata:
    file_path: str          # Ruta del archivo origen
    start_line: int         # LÃ­nea de inicio
    end_line: int           # LÃ­nea de fin
    chunk_type: str         # "function", "class", "text", etc.
    
    # Para cÃ³digo Python
    name: Optional[str]             # Nombre de funciÃ³n/clase
    parent_class: Optional[str]     # Clase padre si es mÃ©todo
    signature: Optional[str]        # Firma de la funciÃ³n
    docstring: Optional[str]        # Docstring si existe
    decorators: List[str]           # Lista de decoradores
    
    # Para documentaciÃ³n
    section_title: Optional[str]    # TÃ­tulo de secciÃ³n
    heading_level: Optional[int]    # Nivel de heading (h1, h2, etc.)
    
    # Trazabilidad
    commit_hash: Optional[str]
    indexed_at: datetime
```

### 8.3 Query y Respuesta

```python
@dataclass
class Query:
    text: str               # Pregunta del usuario
    top_k: int = 5          # NÃºmero de chunks a recuperar
    filters: Dict = None    # Filtros opcionales (por archivo, tipo, etc.)

@dataclass
class RetrievedChunk:
    chunk: Chunk
    similarity_score: float
    rank: int

@dataclass
class Citation:
    file_path: str
    start_line: int
    end_line: int
    
    def __str__(self):
        return f"[{self.file_path}:{self.start_line}-{self.end_line}]"

@dataclass
class Response:
    answer: str                         # Respuesta generada
    citations: List[Citation]           # Lista de citas usadas
    retrieved_chunks: List[RetrievedChunk]  # Chunks recuperados
    confidence: float                   # Confianza en la respuesta (0-1)
    grounded: bool                      # True si estÃ¡ fundamentada en chunks
```

---

## 9. Estrategia de EvaluaciÃ³n

### 9.1 Prueba Cerrada (Respuestas en Docs)

**Objetivo**: Verificar que el sistema recupera el chunk correcto y la respuesta es fiel al texto.

**MetodologÃ­a**:
1. Crear set de preguntas donde la respuesta estÃ¡ literalmente en el repo
2. Para cada pregunta, definir:
   - Archivo(s) esperado(s)
   - Rango de lÃ­neas esperado
   - Contenido clave que debe aparecer en la respuesta

**MÃ©tricas**:
- **Retrieval Accuracy**: Â¿El chunk correcto estÃ¡ en top-k?
- **Faithfulness**: Â¿La respuesta se mantiene fiel al contenido recuperado?
- **Citation Accuracy**: Â¿Las citas apuntan a los lugares correctos?

### 9.2 Prueba Abierta (Fuera de Docs)

**Objetivo**: Verificar que el modelo dice "no estÃ¡ en la base" cuando corresponde.

**MetodologÃ­a**:
1. Crear preguntas sobre cosas que NO estÃ¡n en el repo
2. El sistema debe responder indicando que no tiene informaciÃ³n

**MÃ©tricas**:
- **Abstention Rate**: Â¿Con quÃ© frecuencia se abstiene correctamente?
- **Hallucination Detection**: Â¿Inventa informaciÃ³n cuando no deberÃ­a?

### 9.3 Dataset de EvaluaciÃ³n (Formato JSONL)

```jsonl
{"id": "q001", "type": "closed", "question": "Â¿DÃ³nde se define la funciÃ³n authenticate_user?", "expected_files": ["src/auth/handlers.py"], "expected_line_range": [45, 78], "expected_keywords": ["authenticate_user", "password", "hash"]}
{"id": "q002", "type": "closed", "question": "Â¿QuÃ© parÃ¡metros recibe process_payment?", "expected_files": ["src/payments/processor.py"], "expected_line_range": [112, 145], "expected_keywords": ["amount", "currency", "card_token"]}
{"id": "q003", "type": "open", "question": "Â¿CÃ³mo se conecta a MongoDB?", "expected_behavior": "abstain", "reason": "No hay conexiÃ³n a MongoDB en este repo"}
{"id": "q004", "type": "open", "question": "Â¿CuÃ¡l es el endpoint para eliminar usuarios?", "expected_behavior": "abstain", "reason": "No existe endpoint DELETE /users"}
```

### 9.4 Framework de EvaluaciÃ³n Automatizada

**MÃ©tricas a implementar**:

1. **Retrieval Metrics**:
   - Precision@K
   - Recall@K
   - MRR (Mean Reciprocal Rank)

2. **Generation Metrics**:
   - Faithfulness Score (usando modelo evaluador)
   - Answer Relevancy
   - Citation Precision/Recall

3. **End-to-End Metrics**:
   - Correctness (respuesta correcta)
   - Groundedness (fundamentada en contexto)
   - Abstention Accuracy (se abstiene cuando debe)

---

## 10. Fine-tuning para Comportamiento Grounded

### 10.1 Objetivo del Fine-tune

**NO** entrenar conocimiento del repo (eso viene del retrieval).
**SÃ** entrenar el comportamiento:
- Incluir citas por afirmaciÃ³n
- Responder en formato estructurado
- Abstenerse cuando no hay evidencia

### 10.2 TÃ©cnica: QLoRA

**Por quÃ© QLoRA**:
- Permite ajustar modelo cuantizado (4-bit)
- Solo entrena adaptadores LoRA (~100MB de salida)
- Baja memoria, viable en RTX 4060 8GB
- No toca todos los pesos del modelo base

**ConfiguraciÃ³n tÃ­pica**:

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,                    # Rango de las matrices LoRA
    lora_alpha=32,           # Factor de escalado
    target_modules=[         # QuÃ© mÃ³dulos adaptar
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

### 10.3 Dataset de Entrenamiento (Formato)

```jsonl
{
  "instruction": "Responde la siguiente pregunta basÃ¡ndote ÃšNICAMENTE en el contexto proporcionado. Incluye citas en formato [archivo:lÃ­neas] para cada afirmaciÃ³n.",
  "context": "[Chunk 1: src/auth/handlers.py:45-78]\ndef authenticate_user(self, username: str, password: str) -> bool:\n    \"\"\"Authenticate user with username and password.\"\"\"\n    hashed = hashlib.sha256(password.encode()).hexdigest()\n    return self.db.verify_credentials(username, hashed)\n\n[Chunk 2: src/models/user.py:12-35]\nclass User:\n    def __init__(self, username, email):\n        self.username = username\n        self.email = email",
  "question": "Â¿CÃ³mo se verifica la contraseÃ±a del usuario?",
  "response": "La verificaciÃ³n de contraseÃ±a se realiza en la funciÃ³n `authenticate_user` [src/auth/handlers.py:45-78]:\n\n- Se hashea la contraseÃ±a usando SHA-256 [src/auth/handlers.py:48]\n- Se verifica contra la base de datos llamando a `db.verify_credentials()` [src/auth/handlers.py:49]\n\nEl hash se genera con `hashlib.sha256(password.encode()).hexdigest()`."
}
```

```jsonl
{
  "instruction": "Responde la siguiente pregunta basÃ¡ndote ÃšNICAMENTE en el contexto proporcionado. Si la informaciÃ³n no estÃ¡ disponible, indÃ­calo claramente.",
  "context": "[Chunk 1: src/auth/handlers.py:45-78]\ndef authenticate_user(...)...\n[Chunk 2: src/models/user.py:12-35]\nclass User:...",
  "question": "Â¿CÃ³mo se envÃ­an notificaciones por email?",
  "response": "No encontrÃ© informaciÃ³n sobre el envÃ­o de notificaciones por email en el contexto proporcionado. Los chunks recuperados solo contienen informaciÃ³n sobre autenticaciÃ³n de usuarios y el modelo User, pero no hay cÃ³digo relacionado con notificaciones o emails."
}
```

### 10.4 Contrato de Comportamiento Post Fine-tune

El modelo fine-tuneado debe:

1. **Siempre citar**: Cada afirmaciÃ³n factual â†’ cita
2. **Formato consistente**: `[archivo:lÃ­nea_inicio-lÃ­nea_fin]`
3. **Abstenerse apropiadamente**: Si no hay evidencia â†’ decir que no estÃ¡
4. **No alucinar**: Nunca inventar archivos, funciones o cÃ³digo
5. **Ser conciso**: Respuestas estructuradas, no verbosas

### 10.5 Artefactos de Fine-tuning (NO publicar)

El entrenamiento QLoRA genera estos archivos que **NO deben subirse a Git**:

```
adapters/
â”œâ”€â”€ qwen-coder-grounded/
â”‚   â”œâ”€â”€ adapter_model.safetensors  # ~100MB - NO publicar
â”‚   â”œâ”€â”€ adapter_config.json        # Config del adaptador
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ special_tokens_map.json
```

**En el repo SÃ publicar**:
- `scripts/train_qlora.py` - Script de entrenamiento
- `configs/qlora_config.yaml` - HiperparÃ¡metros
- `README.md` con mÃ©tricas antes/despuÃ©s del fine-tune

---

## 11. Stack TecnolÃ³gico

### 11.1 Backend / Core

| Componente | TecnologÃ­a | JustificaciÃ³n |
|------------|------------|---------------|
| Framework | FastAPI | Async, moderno, buena documentaciÃ³n |
| OrquestaciÃ³n RAG | LangChain o LlamaIndex | Ecosistema maduro, integraciones |
| Parser Python | Tree-sitter (py-tree-sitter) | Chunking semÃ¡ntico preciso |
| **Embeddings** | **nomic-embed-text v1.5** | Local, Apache 2.0, 768 dims |
| **Vector DB** | **ChromaDB** | Simple para MVP, buena DX |
| **LLM** | **Qwen2.5-Coder-7B-Instruct** | Local, Apache 2.0, especializado en cÃ³digo |

### 11.2 Dependencias Python Principales

```toml
[project]
dependencies = [
    # Web Framework
    "fastapi>=0.100.0",
    "uvicorn>=0.23.0",
    "pydantic>=2.0.0",
    
    # UI
    "gradio>=4.0.0",
    
    # LLM Local
    "transformers>=4.40.0",
    "accelerate>=0.27.0",
    "bitsandbytes>=0.43.0",  # CuantizaciÃ³n 4-bit
    "torch>=2.2.0",
    
    # Embeddings
    "sentence-transformers>=2.5.0",
    
    # Vector DB
    "chromadb>=0.4.0",
    
    # RAG (opcional, para utilidades)
    "langchain>=0.1.0",
    "langchain-community>=0.0.20",
    
    # Code Parsing
    "tree-sitter>=0.20.0",
    "tree-sitter-python>=0.20.0",
    
    # Git
    "gitpython>=3.1.0",
    
    # Utils
    "tiktoken>=0.5.0",
    "python-dotenv>=1.0.0",
    "httpx>=0.25.0",
]

[project.optional-dependencies]
finetune = [
    "peft>=0.10.0",           # LoRA/QLoRA
    "trl>=0.8.0",             # Trainer para LLMs
    "datasets>=2.18.0",
    "wandb>=0.16.0",          # Logging (opcional)
]

dev = [
    "pytest>=7.0.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
]
```

### 11.3 Estructura de Proyecto

```
coderag/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ coderag/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py                 # FastAPI app + Gradio mount
â”‚       â”œâ”€â”€ config.py               # ConfiguraciÃ³n (env vars, defaults)
â”‚       â”‚
â”‚       â”œâ”€â”€ ui/                     # Interfaz Gradio
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ app.py              # DefiniciÃ³n de la UI Gradio
â”‚       â”‚   â”œâ”€â”€ components.py       # Componentes reutilizables
â”‚       â”‚   â””â”€â”€ handlers.py         # Handlers de eventos (indexar, preguntar)
â”‚       â”‚
â”‚       â”œâ”€â”€ ingestion/              # Carga y procesamiento de repos
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ loader.py           # Carga de repositorios (clone GitHub)
â”‚       â”‚   â”œâ”€â”€ filter.py           # Filtrado de archivos
â”‚       â”‚   â”œâ”€â”€ chunker.py          # Chunking (AST + texto)
â”‚       â”‚   â””â”€â”€ validator.py        # ValidaciÃ³n de URLs GitHub
â”‚       â”‚
â”‚       â”œâ”€â”€ indexing/               # Embeddings y vector DB
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ embeddings.py       # GeneraciÃ³n con nomic-embed
â”‚       â”‚   â””â”€â”€ vectorstore.py      # InteracciÃ³n con ChromaDB
â”‚       â”‚
â”‚       â”œâ”€â”€ retrieval/              # RecuperaciÃ³n de chunks
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ retriever.py        # LÃ³gica de retrieval
â”‚       â”‚   â””â”€â”€ reranker.py         # Reranking (futuro)
â”‚       â”‚
â”‚       â”œâ”€â”€ generation/             # GeneraciÃ³n de respuestas
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ generator.py        # GeneraciÃ³n con Qwen2.5-Coder
â”‚       â”‚   â”œâ”€â”€ prompts.py          # Templates de prompts
â”‚       â”‚   â””â”€â”€ citations.py        # Parsing/formateo de citas
â”‚       â”‚
â”‚       â”œâ”€â”€ evaluation/             # Framework de evaluaciÃ³n
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ metrics.py          # MÃ©tricas de evaluaciÃ³n
â”‚       â”‚   â”œâ”€â”€ evaluator.py        # Evaluador principal
â”‚       â”‚   â””â”€â”€ datasets.py         # Carga de datasets de eval
â”‚       â”‚
â”‚       â””â”€â”€ api/                    # Endpoints API REST
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ routes.py           # Rutas de API
â”‚           â””â”€â”€ schemas.py          # Schemas Pydantic
â”‚
â”œâ”€â”€ scripts/                        # Scripts de utilidad
â”‚   â”œâ”€â”€ train_qlora.py              # Script de fine-tuning (SÃ publicar)
â”‚   â”œâ”€â”€ evaluate.py                 # Script de evaluaciÃ³n
â”‚   â””â”€â”€ download_models.py          # Pre-descarga de modelos
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_chunker.py
â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â”œâ”€â”€ test_generator.py
â”‚   â”œâ”€â”€ test_validator.py
â”‚   â””â”€â”€ fixtures/
â”‚
â”œâ”€â”€ eval_datasets/                  # Datasets de evaluaciÃ³n (SÃ publicar)
â”‚   â”œâ”€â”€ closed_questions.jsonl
â”‚   â””â”€â”€ open_questions.jsonl
â”‚
â”œâ”€â”€ configs/                        # Configuraciones (SÃ publicar)
â”‚   â”œâ”€â”€ default.yaml
â”‚   â”œâ”€â”€ models.yaml
â”‚   â”œâ”€â”€ filters.yaml
â”‚   â””â”€â”€ qlora_config.yaml
â”‚
â”œâ”€â”€ data/                           # ChromaDB (NO publicar - volumen Docker)
â”œâ”€â”€ repos/                          # Cache repos (NO publicar - volumen Docker)
â”œâ”€â”€ models/                         # Cache HF (NO publicar - volumen Docker)
â”œâ”€â”€ adapters/                       # LoRA adapters (NO publicar - volumen Docker)
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example                    # Ejemplo de .env (SÃ publicar)
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore                      # Excluye data/, repos/, models/, adapters/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                       # Con secciÃ³n "Model Licenses"
```

---

## 12. API Endpoints (MVP)

### 12.1 IndexaciÃ³n

```http
POST /api/v1/repos/index
Content-Type: application/json

{
  "source": "https://github.com/user/repo.git",
  "branch": "main",
  "filters": {
    "include": ["*.py", "*.md"],
    "exclude": ["tests/**"]
  }
}

Response 202:
{
  "job_id": "uuid",
  "status": "processing",
  "message": "Repository indexing started"
}
```

### 12.2 Consulta (Q&A)

```http
POST /api/v1/query
Content-Type: application/json

{
  "question": "Â¿DÃ³nde se define la autenticaciÃ³n de usuarios?",
  "repo_id": "uuid",
  "top_k": 5,
  "include_chunks": true
}

Response 200:
{
  "answer": "La autenticaciÃ³n de usuarios se define en el mÃ³dulo `auth` [src/auth/handlers.py:45-78]...",
  "citations": [
    {
      "file": "src/auth/handlers.py",
      "start_line": 45,
      "end_line": 78,
      "snippet": "def authenticate_user(...)..."
    }
  ],
  "confidence": 0.92,
  "grounded": true,
  "retrieved_chunks": [...]
}
```

### 12.3 Estado de Repositorios

```http
GET /api/v1/repos

Response 200:
{
  "repositories": [
    {
      "id": "uuid",
      "url": "https://github.com/user/repo.git",
      "branch": "main",
      "indexed_at": "2024-01-15T10:30:00Z",
      "chunk_count": 342,
      "status": "ready"
    }
  ]
}
```

---

## 13. Plan de Desarrollo por Fases

### Fase 1: Setup + Docker + Core Pipeline (Semana 1-2)

**Objetivos**:
- [ ] Setup del proyecto (estructura, dependencias, configs)
- [ ] Dockerfile con soporte CUDA y docker-compose.yml funcionales
- [ ] Descarga y configuraciÃ³n de modelos locales (Qwen2.5-Coder + nomic-embed)
- [ ] MÃ³dulo de carga de repositorios (Git clone pÃºblico)
- [ ] ValidaciÃ³n de URLs de GitHub
- [ ] Filtrado de archivos con reglas por defecto
- [ ] Chunking bÃ¡sico por texto
- [ ] IntegraciÃ³n con ChromaDB
- [ ] Embedding con nomic-embed-text e indexaciÃ³n bÃ¡sica
- [ ] VolÃºmenes Docker para persistencia

**Entregable**: `docker compose up` levanta el servicio y puede indexar un repo.

### Fase 2: Interfaz Gradio + Progreso (Semana 2-3)

**Objetivos**:
- [ ] UI Gradio bÃ¡sica con tabs (Indexar / Preguntar)
- [ ] Campo de URL + botÃ³n Indexar
- [ ] Barra de progreso durante indexaciÃ³n (`gr.Progress`)
- [ ] Opciones avanzadas (branch, top-k, filtros)
- [ ] Montar Gradio en FastAPI (`mount_gradio_app`)
- [ ] Feedback de estado (Ã©xito/error)

**Entregable**: UI funcional donde el usuario puede pegar URL e indexar.

### Fase 3: Chunking SemÃ¡ntico Python (Semana 3-4)

**Objetivos**:
- [ ] Integrar Tree-sitter para Python
- [ ] Extraer funciones y clases como chunks
- [ ] Enriquecer metadatos (signature, docstring, decorators)
- [ ] Fallback a chunking por texto para otros archivos
- [ ] Tests unitarios del chunker

**Entregable**: Chunks semÃ¡nticos de calidad para archivos Python.

### Fase 4: GeneraciÃ³n con Citas + Chat UI (Semana 4-5)

**Objetivos**:
- [ ] Implementar mÃ³dulo de generaciÃ³n con Qwen2.5-Coder (4-bit)
- [ ] Sistema de prompts para comportamiento grounded
- [ ] Parsing y formateo de citas `[archivo:lÃ­neas]`
- [ ] Tab "Preguntar" funcional en Gradio
- [ ] Mostrar respuesta + evidencia (chunks usados)
- [ ] Manejo de casos "no encontrado"

**Entregable**: Sistema funcional de Q&A con citas en UI.

### Fase 5: EvaluaciÃ³n y Refinamiento (Semana 5-6)

**Objetivos**:
- [ ] Crear dataset de evaluaciÃ³n (cerradas + abiertas)
- [ ] Implementar mÃ©tricas de evaluaciÃ³n
- [ ] Benchmark del sistema
- [ ] Identificar Ã¡reas de mejora
- [ ] DocumentaciÃ³n (README, docstrings)
- [ ] Demo video para portafolio

**Entregable**: Framework de evaluaciÃ³n funcional + documentaciÃ³n completa.

### Fase 6: Fine-tuning QLoRA (Semana 6+)

**Objetivos**:
- [ ] Preparar dataset de fine-tuning (ejemplos con citas)
- [ ] Configurar entrenamiento QLoRA
- [ ] Entrenar adaptador para comportamiento grounded
- [ ] Evaluar mejora post fine-tune
- [ ] Documentar resultados (mÃ©tricas antes/despuÃ©s)
- [ ] (Opcional) Optimizar retrieval (reranking, hybrid search)

**Entregable**: Sistema optimizado listo para demo/portafolio + adaptador funcional.

---

## 14. Consideraciones Adicionales

### 14.1 Manejo de Errores

- Repositorio no accesible â†’ Error claro con instrucciones
- Parser falla â†’ Fallback a chunking por texto
- LLM no responde â†’ Retry con backoff exponencial
- Chunks insuficientes â†’ Indicar confianza baja
- GPU sin memoria â†’ Mensaje claro, sugerir modelo mÃ¡s ligero

### 14.2 ConfiguraciÃ³n por Entorno

```yaml
# configs/default.yaml
ingestion:
  chunk_size: 1500
  chunk_overlap: 150
  max_file_size_kb: 500

retrieval:
  top_k: 5
  similarity_threshold: 0.7

generation:
  model: "Qwen/Qwen2.5-Coder-7B-Instruct"
  quantization: "4bit"
  temperature: 0.1
  max_new_tokens: 1024

embeddings:
  model: "nomic-ai/nomic-embed-text-v1.5"
  dimensions: 768
  normalize: true
```

### 14.3 Logging y Observabilidad

- Logs estructurados (JSON) con contexto de request
- MÃ©tricas de latencia por componente
- Tracking de uso de VRAM
- Trazas de retrieval para debugging

### 14.4 Seguridad

- No indexar archivos con secretos (.env, credentials)
- Sanitizar inputs de usuario
- Rate limiting en API
- Los volÃºmenes Docker quedan fuera del repo

---

## 15. Glosario

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **RAG** | Retrieval-Augmented Generation: tÃ©cnica que aumenta prompts con informaciÃ³n recuperada |
| **Grounded** | Respuestas fundamentadas en evidencia, no alucinaciones |
| **Chunk** | Fragmento de texto/cÃ³digo indexado |
| **Embedding** | RepresentaciÃ³n vectorial de un texto |
| **Top-k** | Los K resultados mÃ¡s relevantes en una bÃºsqueda |
| **QLoRA** | Quantized Low-Rank Adaptation: fine-tuning eficiente en memoria |
| **LoRA Adapter** | PequeÃ±o conjunto de pesos que adapta un modelo base a una tarea |
| **AST** | Abstract Syntax Tree: representaciÃ³n estructural del cÃ³digo |
| **Tree-sitter** | Parser incremental para anÃ¡lisis de cÃ³digo |
| **Faithfulness** | MÃ©trica de quÃ© tan fiel es la respuesta al contexto |
| **Abstention** | Cuando el modelo se rehÃºsa a responder por falta de evidencia |
| **ChromaDB** | Base de datos vectorial open source para embeddings |
| **nomic-embed** | Modelo de embeddings open source bajo Apache 2.0 |
| **Qwen2.5-Coder** | LLM de Alibaba especializado en cÃ³digo, Apache 2.0 |
| **Gradio** | Framework Python para crear interfaces web para ML/AI |
| **FastAPI** | Framework web moderno y async para Python |
| **Docker Compose** | Herramienta para definir y ejecutar aplicaciones multi-contenedor |

---

## 16. Referencias y Recursos

### DocumentaciÃ³n TÃ©cnica
- [LangChain Documentation](https://python.langchain.com/)
- [LlamaIndex Documentation](https://docs.llamaindex.ai/)
- [Tree-sitter Documentation](https://tree-sitter.github.io/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Gradio Documentation](https://www.gradio.app/docs/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Docker Documentation](https://docs.docker.com/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [PEFT (LoRA/QLoRA)](https://huggingface.co/docs/peft/)

### Modelos
- [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)
- [nomic-embed-text v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)
- [Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) (alternativa)

### Papers Relevantes
- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al.)
- "LoRA: Low-Rank Adaptation of Large Language Models" (Hu et al.)
- "QLoRA: Efficient Finetuning of Quantized LLMs" (Dettmers et al.)

### Tutoriales Recomendados
- [Gradio + FastAPI Integration](https://www.gradio.app/guides/sharing-your-app#api-page)
- [Gradio Progress Bars](https://www.gradio.app/docs/gradio/progress)
- [QLoRA Fine-tuning Guide](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

---

*Documento generado para servir como contexto completo del proyecto CodeRAG.*
*Ãšltima actualizaciÃ³n: Diciembre 2024*
*Modelos: Qwen2.5-Coder-7B-Instruct (LLM) + nomic-embed-text v1.5 (Embeddings)*
*Vector DB: ChromaDB*
